{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Do not restart the runtime after running this cell, it is just a warning everything will work fine\n",
        "!pip install TTS transformers torchaudio\n",
        "!pip install gTTS"
      ],
      "metadata": {
        "id": "qCtLj-mvScOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete SSL-TTS Implementatio#n\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.nn as nn\n",
        "from transformers import WavLMModel\n",
        "from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n",
        "from TTS.tts.models.glow_tts import GlowTTS\n",
        "import torch.nn.functional as F\n",
        "from typing import List\n",
        "import os\n"
      ],
      "metadata": {
        "id": "cxCbzP35SPTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SSLEncoder:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        print(f\"Loading WavLM model to {device}...\")\n",
        "        self.model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)\n",
        "        self.model.eval()\n",
        "        print(\"WavLM model loaded successfully!\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_features(self, waveform, sample_rate=16000):\n",
        "        \"\"\"Extract WavLM features from the 6th layer\"\"\"\n",
        "        # Resample if sample rate is not 16000 Hz\n",
        "        if sample_rate != 16000:\n",
        "            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "        # Ensure waveform is properly batched\n",
        "        if waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # Move waveform to the specified device\n",
        "        waveform = waveform.to(self.device)\n",
        "        outputs = self.model(waveform, output_hidden_states=True)\n",
        "\n",
        "        # Extract features from the 6th layer\n",
        "        features = outputs.hidden_states[6]\n",
        "        return features"
      ],
      "metadata": {
        "id": "wA7TlGlhSS4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextToSSL(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        # Initialize GlowTTS config with proper dimensions\n",
        "        config = GlowTTSConfig(\n",
        "            num_chars=148,  # Increased for full character set\n",
        "            hidden_channels_enc=192,\n",
        "            hidden_channels_dec=192,\n",
        "            out_channels=1024,  # WavLM feature dimension\n",
        "            use_encoder_prenet=True,\n",
        "            encoder_type=\"rel_pos_transformer\",\n",
        "            dropout_p_dec=0.1,\n",
        "        )\n",
        "\n",
        "        # Initialize GlowTTS model\n",
        "        self.glow_tts = GlowTTS(config).to(self.device)\n",
        "\n",
        "\n",
        "        # Initialize tokenizer\n",
        "        from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "        from TTS.tts.utils.text.characters import Graphemes\n",
        "\n",
        "        self.tokenizer = TTSTokenizer(\n",
        "            use_phonemes=False,\n",
        "            characters=Graphemes(),\n",
        "        )\n",
        "\n",
        "        # Load checkpoint if available\n",
        "        if os.path.exists(\"glowtts_checkpoint.pth\"):\n",
        "            checkpoint = torch.load(\"glowtts_checkpoint.pth\")\n",
        "            self.glow_tts.load_state_dict(checkpoint['model'])\n",
        "\n",
        "        self.glow_tts.eval()\n",
        "\n",
        "    def tokenize_text(self, text):\n",
        "        \"\"\"Convert text to token indices.\"\"\"\n",
        "        token_ids = torch.LongTensor(self.tokenizer.text_to_ids(text)).to(self.device)\n",
        "        return token_ids\n",
        "\n",
        "    def generate(self, text: List[str], text_lengths=None):\n",
        "        \"\"\"Generate SSL features from text input.\"\"\"\n",
        "        # Tokenize all text inputs\n",
        "        tokenized_texts = [self.tokenize_text(t) for t in text]\n",
        "\n",
        "        # Calculate lengths if not provided\n",
        "        if text_lengths is None:\n",
        "            text_lengths = torch.tensor([len(t) for t in tokenized_texts]).long().to(self.device)\n",
        "\n",
        "        # Pad sequences to max length\n",
        "        max_length = max(text_lengths)\n",
        "        padded_texts = []\n",
        "        for tokens in tokenized_texts:\n",
        "            if len(tokens) < max_length:\n",
        "                padding = torch.zeros(max_length - len(tokens), dtype=torch.long, device=self.device)\n",
        "                tokens = torch.cat([tokens, padding])\n",
        "            padded_texts.append(tokens)\n",
        "\n",
        "        # Stack into batch\n",
        "        text_tensor = torch.stack(padded_texts)\n",
        "\n",
        "        # Generate features using GlowTTS\n",
        "        outputs = self.glow_tts.inference(\n",
        "            text_tensor,\n",
        "            aux_input={\"x_lengths\": text_lengths}\n",
        "        )\n",
        "        return outputs['model_outputs']"
      ],
      "metadata": {
        "id": "5VK8JVOxVlct"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TTSPipeline(nn.Module):\n",
        "    def __init__(self, source='LJSpeech', target='LJSpeech'):\n",
        "        super().__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.ssl_encoder = SSLEncoder()\n",
        "        self.text_to_ssl = TextToSSL()\n",
        "        # Initialize the vocoder by getting it directly from knn_vc\n",
        "        knn_vc = torch.hub.load(\n",
        "            'bshall/knn-vc',\n",
        "            'knn_vc',\n",
        "            pretrained=True,\n",
        "            prematched=True,\n",
        "            trust_repo=True\n",
        "        )\n",
        "        self.vocoder = knn_vc.hifigan\n",
        "        self.i = 0\n",
        "\n",
        "    def get_features(self, path=None, waveform=None, get_target=False):\n",
        "        if waveform is not None:\n",
        "            x = waveform\n",
        "        else:\n",
        "            x, sample_rate = torchaudio.load(path, normalize=True)\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "                x = resampler(x)\n",
        "\n",
        "        if get_target:\n",
        "            transform = torchaudio.transforms.Vad(sample_rate=16000, trigger_level=7.0)\n",
        "            x_trim = transform(x)\n",
        "            x_reversed = torch.flip(x_trim, (-1,))\n",
        "            x_reversed_trim = transform(x_reversed)\n",
        "            x_full_trim = torch.flip(x_reversed_trim, (-1,))\n",
        "            x = x_full_trim\n",
        "\n",
        "        features = self.ssl_encoder.extract_features(x, sample_rate=16000)\n",
        "        features = features.squeeze(0) if features.dim() > 2 else features\n",
        "        return features.to(self.device)\n",
        "\n",
        "    def get_target_features(self, wavs):\n",
        "        \"\"\"Get features from target speaker utterances.\"\"\"\n",
        "        if isinstance(wavs, str):\n",
        "            wavs = [wavs]\n",
        "        features = []\n",
        "        for path in wavs:\n",
        "            feat = self.get_features(path, get_target=True)\n",
        "            features.append(feat)\n",
        "        features = torch.cat(features, dim=0)\n",
        "        return features.to(self.device)\n",
        "\n",
        "    def cosine_dist(self, source_features, target_features):\n",
        "        source_norms = torch.norm(source_features, p=2, dim=-1)\n",
        "        matching_norms = torch.norm(target_features, p=2, dim=-1)\n",
        "        dotprod = -torch.cdist(source_features[None], target_features[None], p=2)[0]**2 + \\\n",
        "                  source_norms[:, None]**2 + matching_norms[None]**2\n",
        "        dotprod /= 2\n",
        "        dists = 1 - (dotprod / (source_norms[:, None] * matching_norms[None]))\n",
        "        return dists\n",
        "\n",
        "    def KNN(self, source_features, target_features):\n",
        "        synth_set = target_features\n",
        "        dists = self.cosine_dist(source_features, target_features)\n",
        "        best = dists.topk(k=4, largest=False, dim=-1)\n",
        "        selected_features = synth_set[best.indices].mean(dim=1)\n",
        "        return selected_features\n",
        "\n",
        "    def forward(self, text, target_wavs, save_file=False, lambda_value=1.0):\n",
        "        \"\"\"Generate speech given text input and target speaker references.\"\"\"\n",
        "        # Convert text to SSL features using GlowTTS\n",
        "        source_features = self.text_to_ssl.generate([text])\n",
        "        print(f\"Source features initial shape: {source_features.shape}\")\n",
        "\n",
        "        # Get target speaker features\n",
        "        target_features = self.get_target_features(target_wavs)\n",
        "        print(f\"Target features shape: {target_features.shape}\")\n",
        "\n",
        "        # Ensure source_features has the right shape (sequence_length, feature_dim)\n",
        "        if source_features.dim() == 3:\n",
        "            source_features = source_features.squeeze(0)\n",
        "        print(f\"Source features shape before KNN: {source_features.shape}\")\n",
        "\n",
        "        # Perform KNN retrieval\n",
        "        selected_features = self.KNN(source_features, target_features)\n",
        "        print(f\"Selected features shape: {selected_features.shape}\")\n",
        "\n",
        "        # Linear interpolation\n",
        "        converted_features = lambda_value * selected_features + (1 - lambda_value) * source_features\n",
        "\n",
        "        # Following the working example, the vocoder expects shape [batch_size, channels, seq_len]\n",
        "        # No need for additional reshaping beyond transposing\n",
        "        converted_features = converted_features.unsqueeze(0)  # Add batch dimension\n",
        "        print(f\"Converted features shape for vocoder: {converted_features.shape}\")\n",
        "\n",
        "        # Generate waveform using HiFi-GAN\n",
        "        generated_waveform = self.vocoder(converted_features.to(self.device)).squeeze()\n",
        "\n",
        "        # Normalize loudness\n",
        "        src_loudness = torchaudio.functional.loudness(generated_waveform[None], 16000)\n",
        "        tgt_loudness = -16\n",
        "        generated_waveform = torchaudio.functional.gain(generated_waveform,\n",
        "                                                      tgt_loudness - src_loudness)\n",
        "\n",
        "        if save_file:\n",
        "            save_waveform = generated_waveform\n",
        "            if self.i < 10:\n",
        "                output_path = f'/content/generated_waveform_0{self.i}.wav'\n",
        "            else:\n",
        "                output_path = f'/content/generated_waveform_{self.i}.wav'\n",
        "            torchaudio.save(output_path, save_waveform.unsqueeze(0).cpu(), sample_rate=16000)\n",
        "            print(f\"Generated waveform saved at {output_path}\")\n",
        "            self.i += 1  # Increment self.i after saving the waveform\n",
        "\n",
        "        return generated_waveform.unsqueeze(0)"
      ],
      "metadata": {
        "id": "VRcBSwVQSX0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exampe Usage\\\n",
        "The TextToSSL (GlowTTS) needs just a text input to generate WavLM features. The TTSPipeline takes in text and a target audio file and produces an audio file. Feel free to load the LJSpeech dataset and use it as a target audio."
      ],
      "metadata": {
        "id": "oLayBcSgbIe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget --no-check-certificate https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Extract the dataset\n",
        "!tar -xjf LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Verify the extraction by listing the contents\n",
        "!ls LJSpeech-1.1"
      ],
      "metadata": {
        "id": "wZYXOMvAbqDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "text_to_ssl = TextToSSL()\n",
        "\n",
        "# Generate features from text\n",
        "text_inputs = [\"Hello world\", \"We've enhanced Connect with new features to manage and pay for multiple users from one account. These features were previously available only in beta. As part of this update, we're also revising our terms and conditions.\"]\n",
        "features = text_to_ssl.generate(text_inputs)"
      ],
      "metadata": {
        "id": "MxNiuqsHX3WD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize pipeline\n",
        "pipeline = TTSPipeline()\n",
        "\n",
        "# Set up target speaker wavs, change to LJSpeech wavs if you decide to use that\n",
        "#target_wavs = [path_to_wav1, path_to_wav2, ...]\n",
        "targ = \"/content/test.wav\"\n",
        "# Generate speech\n",
        "output = pipeline(\"We've enhanced Connect with new features to manage and pay for multiple users from one account. These features were previously available only in beta. As part of this update, we're also revising our terms and conditions.\", targ, save_file=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TxP3uuaS5EC",
        "outputId": "c6a19d78-b826-447a-b061-2b7087d6eb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading WavLM model to cuda...\n",
            "WavLM model loaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/bshall_knn-vc_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing weight norm...\n",
            "[HiFiGAN] Generator loaded with 16,523,393 parameters.\n",
            "WavLM-Large loaded with 315,453,120 parameters.\n",
            "Source features initial shape: torch.Size([1, 220, 1024])\n",
            "Target features shape: torch.Size([301, 1024])\n",
            "Source features shape before KNN: torch.Size([220, 1024])\n",
            "Selected features shape: torch.Size([220, 1024])\n",
            "Converted features shape for vocoder: torch.Size([1, 220, 1024])\n",
            "Generated waveform saved at /content/generated_waveform_00.wav\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}