{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF89d5Vkv3ny"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvGacQEGY8dN"
      },
      "source": [
        "This cell handles the initial setup and imports all necessary dependencies for the SSL-TTS framework:\n",
        "- Clones the TTS repository from coqui-ai\n",
        "- Installs required packages: TTS, transformers, torchaudio\n",
        "- Imports core deep learning libraries (torch, torchaudio)\n",
        "- Imports WavLM model for SSL feature extraction\n",
        "- Imports GlowTTS components for the text-to-SSL model\n",
        "- Sets up other essential utilities like torch.nn.functional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYDZ3C47hNVt",
        "outputId": "18d66a4a-ad6e-4ca0-8e2d-9209384b195e"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/idiap/coqui-ai-TTS.git\n",
        "!pip install transformers torchaudio\n",
        "\n",
        "# DO NOT RESTART RUNTIME AFTER RUNNING THIS CELL\n",
        "# YOU MIGHT HAVE A FEW WARNINGS/ERROR BUT DW IT'S FINE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i_HPtQ6teB1"
      },
      "outputs": [],
      "source": [
        "# prompt: put all necessary imports for this notebook in this cell\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torch.optim as optim\n",
        "from transformers import WavLMModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from torch import nn\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import json\n",
        "import torch.nn.functional as F\n",
        "from typing import Tuple, Dict, List\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from dataclasses import dataclass, field\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5_3cUmif-2"
      },
      "source": [
        "# SSL Encoder Implementation (WavLM Integration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lke3HJVbZDmJ"
      },
      "source": [
        "Implements the Self-Supervised Learning encoder component using WavLM-Large:\n",
        "\n",
        "### Key Features\n",
        "1. Model Initialization:\n",
        "   - Loads WavLM-Large model from HuggingFace\n",
        "   - Automatically selects GPU if available\n",
        "   - Sets model to evaluation mode\n",
        "\n",
        "2. Feature Extraction:\n",
        "   - Uses WavLM's 6th layer for optimal feature representation\n",
        "   - Handles automatic resampling to 16kHz\n",
        "   - Manages proper tensor dimensions and device placement\n",
        "   - Outputs 1024-dimensional feature vectors\n",
        "\n",
        "3. Audio Processing:\n",
        "   - Supports variable length inputs\n",
        "   - Handles mono/stereo conversion\n",
        "   - Implements automatic batching\n",
        "\n",
        "### Technical Details\n",
        "- Input: Audio waveform tensor [B, T] or [1, T]\n",
        "- Output: SSL features [B, T', 1024]\n",
        "- Uses @torch.no_grad() for efficient inference\n",
        "- Includes sample rate verification and conversion\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "collapsed": true,
        "id": "U3onB44pwWbx",
        "outputId": "c6275052-aa2d-4310-af14-319c1f375a9d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'# Example usage\\nssl_encoder = SSLEncoder()\\n\\n# Load a sample audio file (replace \\'path_to_audio_file.wav\\' with the actual file path)\\nwaveform, sample_rate = torchaudio.load(\\'/content/harvard.wav\\')\\n\\n# Extract features\\nfeatures = ssl_encoder.extract_features(waveform, sample_rate)\\nprint(\"Extracted features shape:\", features.shape)'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class SSLEncoder:\n",
        "    def __init__(self, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "        self.device = device\n",
        "        print(f\"Loading WavLM model to {device}...\")\n",
        "        self.model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(device)\n",
        "        self.model.eval()\n",
        "        print(\"WavLM model loaded successfully!\")\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def extract_features(self, waveform, sample_rate=16000):\n",
        "        \"\"\"Extract WavLM features from the 6th layer\"\"\"\n",
        "        # Resample if sample rate is not 16000 Hz\n",
        "        if sample_rate != 16000:\n",
        "            waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "        # Ensure waveform is properly batched\n",
        "        if waveform.ndim == 1:\n",
        "            waveform = waveform.unsqueeze(0)\n",
        "\n",
        "        # Move waveform to the specified device\n",
        "        waveform = waveform.to(self.device)\n",
        "        outputs = self.model(waveform, output_hidden_states=True)\n",
        "\n",
        "        # Extract features from the 6th layer\n",
        "        features = outputs.hidden_states[6]\n",
        "        return features\n",
        "\n",
        "'''# Example usage\n",
        "ssl_encoder = SSLEncoder()\n",
        "\n",
        "# Load a sample audio file (replace 'path_to_audio_file.wav' with the actual file path)\n",
        "waveform, sample_rate = torchaudio.load('/content/harvard.wav')\n",
        "\n",
        "# Extract features\n",
        "features = ssl_encoder.extract_features(waveform, sample_rate)\n",
        "print(\"Extracted features shape:\", features.shape)'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46Zm0FY2r5UJ"
      },
      "source": [
        "#LJSpeech dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s-CY8TcqfFs",
        "outputId": "05cc39d3-22f2-4af7-c01b-b6a9acc79563"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-12-13 14:34:00--  https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
            "Resolving data.keithito.com (data.keithito.com)... 169.150.236.98, 2400:52e0:1a00::1207:2\n",
            "Connecting to data.keithito.com (data.keithito.com)|169.150.236.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2748572632 (2.6G) [text/plain]\n",
            "Saving to: ‘LJSpeech-1.1.tar.bz2’\n",
            "\n",
            "LJSpeech-1.1.tar.bz 100%[===================>]   2.56G   157MB/s    in 16s     \n",
            "\n",
            "2024-12-13 14:34:16 (162 MB/s) - ‘LJSpeech-1.1.tar.bz2’ saved [2748572632/2748572632]\n",
            "\n",
            "metadata.csv  README  wavs\n"
          ]
        }
      ],
      "source": [
        "# Download the LJSpeech dataset\n",
        "!wget https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Extract the dataset\n",
        "!tar -xjf LJSpeech-1.1.tar.bz2\n",
        "\n",
        "# Verify the extraction by listing the contents\n",
        "!ls LJSpeech-1.1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZVo4IxMeAGI"
      },
      "source": [
        "# k-NN Retrieval System"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gITtWrXBXxyB"
      },
      "source": [
        "Implements the k-Nearest Neighbors retrieval mechanism for voice conversion:\n",
        "\n",
        "### Technical Features\n",
        "1. Efficient Batch Processing:\n",
        "   - Handles multiple sequences simultaneously\n",
        "   - Optimized matrix operations\n",
        "   - Memory-efficient implementation\n",
        "\n",
        "2. Distance Calculation (more below):\n",
        "   - Cosine similarity metric\n",
        "   - Numerical stability handling\n",
        "   - Batch matrix multiplication\n",
        "\n",
        "3. Feature Averaging:\n",
        "   - Uniform weighting of k-nearest neighbors\n",
        "   - Proper dimension handling\n",
        "   - Gradient-free operations\n",
        "\n",
        "### Parameters\n",
        "- k: Number of neighbors (default: 4)\n",
        "- device: Computation device\n",
        "- input dimensions: [B, T, D] for both source and target\n",
        "- output dimensions: [B, T, D] for selected features\n",
        "\n",
        "### Cosine Similarity\n",
        "For two feature vectors $\\mathbf{a}$ and $\\mathbf{b}$ in a high-dimensional space (in our case, $\\mathbb{R}^{1024}$), the cosine similarity is defined as:\n",
        "\n",
        "$\n",
        "\\cos(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{\\|\\mathbf{a}\\| \\|\\mathbf{b}\\|}\n",
        "$\n",
        "\n",
        "Where:\n",
        "- $\\mathbf{a} \\cdot \\mathbf{b}$ is the dot product\n",
        "- $\\|\\mathbf{a}\\|$ and $\\|\\mathbf{b}\\|$ are the L2 norms (Euclidean norms)\n",
        "\n",
        "For batched computation with source features $\\mathbf{S} \\in \\mathbb{R}^{B \\times T_s \\times D}$ and target features $\\mathbf{T} \\in \\mathbb{R}^{B \\times T_t \\times D}$, we compute:\n",
        "\n",
        "$\n",
        "\\text{Similarity}_{batch} = \\frac{\\mathbf{S}\\mathbf{T}^T}{\\|\\mathbf{S}\\|_2 \\|\\mathbf{T}\\|_2^T}\n",
        "$\n",
        "\n",
        "### Cosine Distance\n",
        "The cosine distance is derived from the cosine similarity:\n",
        "\n",
        "$\n",
        "d_{cos}(\\mathbf{a}, \\mathbf{b}) = 1 - \\cos(\\mathbf{a}, \\mathbf{b})\n",
        "$\n",
        "\n",
        "In our implementation, we compute this in steps:\n",
        "\n",
        "1. **Dot Product**:\n",
        "   $\\text{dot}_{batch} = \\mathbf{S}\\mathbf{T}^T \\in \\mathbb{R}^{B \\times T_s \\times T_t}$\n",
        "\n",
        "2. **L2 Norms**:\n",
        "   $\\|\\mathbf{S}\\|_2 \\in \\mathbb{R}^{B \\times T_s \\times 1}$ and $\\|\\mathbf{T}\\|_2 \\in \\mathbb{R}^{B \\times T_t \\times 1}$\n",
        "\n",
        "3. **Norm Product**:\n",
        "   $\\text{norm\\_prod} = \\|\\mathbf{S}\\|_2\\|\\mathbf{T}\\|_2^T \\in \\mathbb{R}^{B \\times T_s \\times T_t}$\n",
        "\n",
        "4. **Final Distance**:\n",
        "   $d_{cos} = 1 - \\frac{\\text{dot}_{batch}}{\\text{norm\\_prod} + \\epsilon}$\n",
        "\n",
        "where $\\epsilon = 1e-8$ for numerical stability.\n",
        "\n",
        "This distance metric has several advantageous properties for our SSL-TTS framework:\n",
        "\n",
        "1. **Bounded Range**: $d_{cos} \\in [0, 2]$ where:\n",
        "   - 0 indicates identical direction\n",
        "   - 1 indicates orthogonal vectors\n",
        "   - 2 indicates opposite directions\n",
        "\n",
        "2. **Scale Invariance**: The distance is invariant to the magnitude of the vectors, making it suitable for comparing SSL features that may have different magnitudes but similar patterns.\n",
        "\n",
        "3. **Batch Efficiency**: The formulation allows efficient computation across batches using matrix operations, crucial for processing multiple time steps simultaneously.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_-5vfgQX5SW"
      },
      "source": [
        "# $\\lambda$ function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHy2g6sgdAAY"
      },
      "source": [
        "### Core Implementation\n",
        "1. Interpolation Formula:\n",
        "```python\n",
        "converted_features = lambda_value * selected_features +\n",
        "                    (1 - lambda_value) * source_features\n",
        "```\n",
        "\n",
        "2. Parameter Management:\n",
        "   - Lambda value bounds checking\n",
        "   - Device handling\n",
        "   - Tensor dimension verification\n",
        "\n",
        "### Features\n",
        "1. Input Validation:\n",
        "   - Lambda range enforcement\n",
        "   - Tensor dimension checking\n",
        "   - Device consistency\n",
        "\n",
        "2. Computation Efficiency:\n",
        "   - In-place operations where possible\n",
        "   - Memory-efficient implementation\n",
        "   - Batch processing support\n",
        "\n",
        "3. Interface Options:\n",
        "   - Direct method call\n",
        "   - Callable interface\n",
        "   - Flexible parameter passing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGR_YD6OYPs6"
      },
      "source": [
        "# Vocoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xX5nZOWh8uU"
      },
      "source": [
        "Implements the HiFi-GAN vocoder for waveform generation:\n",
        "\n",
        "### Technical Details\n",
        "1. Model Components:\n",
        "   - Residual blocks\n",
        "   - Upsampling layers\n",
        "   - Convolutional layers\n",
        "\n",
        "2. Audio Generation:\n",
        "   - Feature conditioning\n",
        "   - Multi-scale processing\n",
        "   - Waveform synthesis\n",
        "\n",
        "3. Current Status:\n",
        "   - Checkpoint loading issue\n",
        "   - Needs path configuration\n",
        "   - Testing infrastructure ready"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQsds0npTcdz"
      },
      "outputs": [],
      "source": [
        "knn_vc = torch.hub.load(\n",
        "    'bshall/knn-vc',\n",
        "    'knn_vc',\n",
        "    pretrained=True,\n",
        "    prematched=True,\n",
        "    trust_repo=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pW5qXOAYRXBy"
      },
      "outputs": [],
      "source": [
        "vocoder = knn_vc.hifigan\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iFY2PeL3O3T"
      },
      "source": [
        "# Urhythmic Component"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AldQKYt73Soy"
      },
      "outputs": [],
      "source": [
        "# Pretrained models are available for:\n",
        "# VCTK: p228, p268, p225, p232, p257, p231.\n",
        "# and LJSpeech.\n",
        "\n",
        "def load_urhythmic_model(source, target):\n",
        "    hubert = torch.hub.load(\"bshall/hubert:main\", \"hubert_soft\").cuda()\n",
        "    urhythmic, encode = torch.hub.load(\n",
        "        \"bshall/urhythmic:main\",\n",
        "        \"urhythmic_global\",\n",
        "        source_speaker=source,\n",
        "        target_speaker=target,\n",
        "    )\n",
        "    urhythmic.cuda()\n",
        "\n",
        "    return hubert, encode, urhythmic\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXTB_ROkHUYQ"
      },
      "source": [
        "# Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUn8n-vUJEKv"
      },
      "outputs": [],
      "source": [
        "!pip install gTTS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w82ccN0GJE0E"
      },
      "outputs": [],
      "source": [
        "from gtts import gTTS\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVo3w-KlSBS4"
      },
      "outputs": [],
      "source": [
        "def cosine_dist(source_features, target_features):\n",
        "    source_norms = torch.norm(source_features, p=2, dim=-1)\n",
        "    matching_norms = torch.norm(target_features, p=2, dim=-1)\n",
        "    dotprod = -torch.cdist(source_features[None], target_features[None], p=2)[0]**2 + source_norms[:, None]**2 + matching_norms[None]**2\n",
        "    dotprod /= 2\n",
        "\n",
        "    dists = 1 - ( dotprod / (source_norms[:, None] * matching_norms[None]) )\n",
        "    return dists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_lCP04iHheo"
      },
      "outputs": [],
      "source": [
        "class TTSPipeline(nn.Module):\n",
        "    def __init__(self, source='LJSpeech', target='LJSpeech'):\n",
        "        super().__init__()\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.ssl_encoder = SSLEncoder()\n",
        "        self.vocoder = vocoder\n",
        "        self.resampler = torchaudio.transforms.Resample(orig_freq=24000, new_freq=16000)\n",
        "        self.hubert, self.encode, self.urhythmic = load_urhythmic_model(source, target)\n",
        "        self.i = 0\n",
        "\n",
        "    def reset_i(self):\n",
        "        self.i = 0\n",
        "\n",
        "    def text_to_waveform(self, text):\n",
        "        #text to speech through google python library\n",
        "        tts = gTTS(text)\n",
        "        #save to buffer\n",
        "        mp3_buffer = BytesIO()\n",
        "        tts.write_to_fp(mp3_buffer)\n",
        "        mp3_buffer.seek(0)\n",
        "        #load\n",
        "        waveform, sample_rate = torchaudio.load(mp3_buffer, format=\"mp3\", normalize=True)\n",
        "        #close and delete buffer\n",
        "        mp3_buffer.close()\n",
        "        del mp3_buffer\n",
        "        #resample\n",
        "        if sample_rate != 16000:\n",
        "            waveform = self.resampler(waveform)\n",
        "\n",
        "        return waveform.to(self.device)\n",
        "\n",
        "    def KNN(self, source_features, target_features):\n",
        "        synth_set = target_features\n",
        "\n",
        "        dists = cosine_dist(source_features, target_features)\n",
        "        best = dists.topk(k=4, largest=False, dim=-1)\n",
        "        selected_features = synth_set[best.indices].mean(dim=1)\n",
        "\n",
        "        return selected_features\n",
        "\n",
        "\n",
        "    def linear_interpolation(self, source_features, selected_features, lambda_value = 1.0):\n",
        "        # Ensure tensors are on correct device\n",
        "        selected_features = selected_features.to(self.device)\n",
        "        source_features = source_features.to(self.device)\n",
        "\n",
        "        # Ensure lambda is in valid range\n",
        "        lambda_value = max(0.0, min(1.0, lambda_value))\n",
        "\n",
        "        # Perform linear interpolation\n",
        "        converted_features = lambda_value * selected_features + (1 - lambda_value) * source_features\n",
        "\n",
        "        return converted_features\n",
        "\n",
        "    def get_target_features(self, wavs):\n",
        "        features = []\n",
        "        # i=0\n",
        "        for path in wavs:\n",
        "            # print(i)\n",
        "            # i+=1\n",
        "            features.append(self.get_features(path, get_target=True))\n",
        "\n",
        "        features = torch.concat(features, dim = 0)\n",
        "        return features.to(self.device)\n",
        "\n",
        "\n",
        "    def get_features(self, path = None, waveform = None, get_target = False):\n",
        "        if waveform is not None:\n",
        "            x = waveform\n",
        "        else:\n",
        "            x, sample_rate = torchaudio.load(path, normalize=True)\n",
        "            if sample_rate != 16000:\n",
        "                resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
        "                x = resampler(x)\n",
        "\n",
        "\n",
        "        if get_target:\n",
        "            transform = torchaudio.transforms.Vad(sample_rate=16000, trigger_level=7.0)\n",
        "            x_trim = transform(x)\n",
        "            x_reversed = torch.flip(x_trim, (-1,))\n",
        "            x_reversed_trim = transform(x_reversed)\n",
        "            x_full_trim = torch.flip(x_reversed_trim, (-1,))\n",
        "            x = x_full_trim\n",
        "\n",
        "        features = self.ssl_encoder.extract_features(x)\n",
        "        features = features.squeeze(0)\n",
        "\n",
        "        return features.to(self.device)\n",
        "\n",
        "    def forward(self, text, wavs, save_file = False):\n",
        "        waveform = self.text_to_waveform(text)\n",
        "\n",
        "        waveform = waveform.to(self.device)\n",
        "\n",
        "        source_features = self.get_features(waveform = waveform)\n",
        "        target_features = self.get_target_features(wavs)\n",
        "\n",
        "        selected_features = self.KNN(source_features, target_features)\n",
        "\n",
        "        converted_features = self.linear_interpolation(source_features, selected_features)\n",
        "\n",
        "        generated_waveform = self.vocoder(selected_features[None].to(self.device)).cpu().squeeze()\n",
        "\n",
        "        src_loudness = torchaudio.functional.loudness(generated_waveform[None], 16000)\n",
        "        tgt_loudness = -16\n",
        "        generated_waveform = torchaudio.functional.gain(generated_waveform, tgt_loudness - src_loudness)\n",
        "\n",
        "        if save_file:\n",
        "            save_waveform = generated_waveform\n",
        "            #change output path to wherever you want to put files in drive\n",
        "            if self.i < 10:\n",
        "                output_path = f'/content/drive/MyDrive/LJ_graph/LJ_150/LJ_150_gtts_generated_waveform_0{self.i}.wav'\n",
        "            else:\n",
        "                output_path = f'/content/drive/MyDrive/LJ_graph/LJ_150/LJ_150_gtts_generated_waveform_{self.i}.wav'\n",
        "            torchaudio.save(output_path, torch.from_numpy(save_waveform.detach().numpy()).unsqueeze(0), sample_rate=16000)\n",
        "            print(f\"Generated waveform saved at {output_path}\")\n",
        "\n",
        "\n",
        "        return generated_waveform.unsqueeze(0)\n",
        "\n",
        "    def forward_with_urhythmic(self, text, wavs, save_file = False, save_example = False):\n",
        "        # set save_example = True to save both the wav file before and after applying\n",
        "        if save_example:\n",
        "            waveform = self.forward(text, wavs, save_file=True)\n",
        "        else:\n",
        "            waveform = self.forward(text, wavs, save_file=False)\n",
        "\n",
        "\n",
        "        waveform = waveform.unsqueeze(0).cuda()\n",
        "        with torch.inference_mode():\n",
        "            # Extract speech units and log probabilities\n",
        "            units, log_probs = self.encode(self.hubert, waveform)\n",
        "            # Convert to the target speaker\n",
        "            generated_waveform = self.urhythmic(units, log_probs)\n",
        "\n",
        "        generated_waveform = generated_waveform.cpu().squeeze()\n",
        "\n",
        "        # src_loudness = torchaudio.functional.loudness(generated_waveform[None], 16000)\n",
        "        # tgt_loudness = -16\n",
        "        # generated_waveform = torchaudio.functional.gain(generated_waveform, tgt_loudness - src_loudness)\n",
        "\n",
        "        if save_file:\n",
        "            save_waveform = generated_waveform\n",
        "            #change output path to wherever you want to put files in drive\n",
        "            if self.i < 10:\n",
        "                output_path = f'/content/drive/MyDrive/LJ_graph/LJ_ur_150/LJ_150_gtts_generated_waveform_urythmic_0{self.i}.wav'\n",
        "            else:\n",
        "                output_path = f'/content/drive/MyDrive/LJ_graph/LJ_ur_150/LJ_150_gtts_generated_waveform_urythmic_{self.i}.wav'\n",
        "            torchaudio.save(output_path, torch.from_numpy(save_waveform.detach().numpy()).unsqueeze(0), sample_rate=16000)\n",
        "            print(f\"Generated waveform saved at {output_path}\")\n",
        "            self.i+=1\n",
        "\n",
        "        return generated_waveform\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeDaJCP4oaZm"
      },
      "source": [
        "# VCTK wavs for Audio Generation (in local drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wee1_bdVOpzE"
      },
      "outputs": [],
      "source": [
        "wavs = [\n",
        "    '/content/drive/MyDrive/p228/p228_001_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_001_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_002_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_003_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_004_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_005_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_006_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_007_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_035_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_009_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_010_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_011_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_012_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_013_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_014_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_033_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_016_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_037_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_018_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_019_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_020_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_021_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_038_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_039_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_024_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_025_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_026_mic2.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_027_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_028_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_029_mic1.flac',\n",
        "    '/content/drive/MyDrive/p228/p228_030_mic1.flac',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQYClojxFMKF"
      },
      "outputs": [],
      "source": [
        "wavs = [\n",
        "    '/content/drive/MyDrive/p231/p231_001_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_001_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_002_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_003_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_004_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_005_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_006_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_007_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_008_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_009_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_010_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_011_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_012_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_013_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_014_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_033_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_016_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_015_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_018_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_019_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_020_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_021_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_031_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_023_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_024_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_025_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_026_mic2.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_027_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_028_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_029_mic1.flac',\n",
        "    '/content/drive/MyDrive/p231/p231_030_mic1.flac',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTJ6jFo8HMpv"
      },
      "outputs": [],
      "source": [
        "wavs = [\n",
        "    '/content/drive/MyDrive/p257/p257_001_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_001_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_002_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_003_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_004_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_005_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_006_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_007_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_008_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_009_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_010_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_011_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_012_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_013_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_014_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_033_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_016_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_015_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_018_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_019_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_020_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_021_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_031_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_023_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_024_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_025_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_026_mic2.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_027_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_028_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_029_mic1.flac',\n",
        "    '/content/drive/MyDrive/p257/p257_030_mic1.flac',\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWBF-KxXKbx1"
      },
      "outputs": [],
      "source": [
        "wavs = [\n",
        "    '/content/drive/MyDrive/p268/p268_001_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_001_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_002_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_003_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_004_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_005_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_006_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_007_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_035_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_009_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_010_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_011_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_012_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_013_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_014_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_033_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_016_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_015_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_018_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_019_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_020_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_021_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_031_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_034_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_024_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_025_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_026_mic2.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_027_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_028_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_029_mic1.flac',\n",
        "    '/content/drive/MyDrive/p268/p268_030_mic1.flac',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQLcBvnBoRvf"
      },
      "source": [
        "# LJSpeech wavs for Audio Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ll6zOQXJbEFi"
      },
      "outputs": [],
      "source": [
        "wavs = [\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0001.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0002.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0003.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0004.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0005.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0006.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0007.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0008.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0009.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0010.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0011.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0012.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0013.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0014.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0015.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0016.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0017.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0018.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0019.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0020.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0021.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0022.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0023.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0024.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0025.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0026.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0027.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0028.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0029.wav',\n",
        "    '/content/LJSpeech-1.1/wavs/LJ001-0030.wav',\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XaeCjjao-GI"
      },
      "source": [
        "# Example Audio Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgd7auZPpCpF"
      },
      "source": [
        "If you wish to test this, the easiest way is to load up the LJSpeech dataset. Just run the cell under LJSpeech dataset as well as the Pipeline cells and the LJSpeech wavs cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546,
          "referenced_widgets": [
            "81311bceba6d47c69e92ab4acca3ffad",
            "29e445fb5d7542b7a674745fc2751210",
            "6d463ca16b64476eb40aa29c649dc94a",
            "41b4387af59a46ed8c9895ba65de23be",
            "d4ac0eb374d0481ba7043e6163ba1502",
            "ea56e1b6309648aab718fac04d75c2df",
            "dcbb88cc507048c0a6cd11a955b77725",
            "5ed898fb83064df69837236c2e22a32d",
            "9169bd3a89d54422a722b9ce513743ba",
            "d8b6d9e60ccf4a7c98e0268ce5b80743",
            "9d2ef5bc7fd841dbb0abb4869af4a325",
            "4ade8b6e915043768fc70bff89846938",
            "4aa2ab1316c74ec0a8321f3ccf893b2c",
            "f44bd06757de43be8117478edde0bf43",
            "d2d4b02220744d90b8ffc8c3599ec0fa",
            "50e8ff7fc43b456ba4ec5709811bef20",
            "d33df9df60064ddd8c75a876ee2cfc6c",
            "af952d4e13cf4f1da608bf5538d5e000",
            "0768730567584a70990f92e698dc2b19",
            "bd4f6eb26d0e4e7d9cb88bd6e87b5f3b",
            "9f407e024f754cb1a5f8df0d811f8f00",
            "2928b6d68d4f45b9aa3c594e7daeb465"
          ]
        },
        "collapsed": true,
        "id": "51qywlYsN73p",
        "outputId": "f3c7c91a-fade-4c08-ed03-3a001c105560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading WavLM model to cpu...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81311bceba6d47c69e92ab4acca3ffad",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ade8b6e915043768fc70bff89846938",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.26G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WavLM model loaded successfully!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'vocoder' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3e91c99b698e>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# df = pd.DataFrame(lines, columns=['text'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Path to the folder where you want to store all audio files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mtts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTTSPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Now, loop through each sentence and create a TTS file for it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-6070177ebf44>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, source, target)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssl_encoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSSLEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresampler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhubert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murhythmic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_urhythmic_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'vocoder' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# If your CSV is on your Google Drive, specify its path.\n",
        "# Example assuming you placed it directly in your \"MyDrive\":\n",
        "# csv_path = '/content/sampled_sentences2.csv'\n",
        "\n",
        "# # Read the CSV file. If it has no header, set header=None.\n",
        "# with open(csv_path, 'r', encoding='utf-8') as file:\n",
        "#     lines = [line.strip() for line in file]\n",
        "\n",
        "# df = pd.DataFrame(lines, columns=['text'])\n",
        "# Path to the folder where you want to store all audio files\n",
        "tts = TTSPipeline()\n",
        "\n",
        "# Now, loop through each sentence and create a TTS file for it\n",
        "for i, row in df.iterrows():\n",
        "    # Each row is just one sentence in the first column\n",
        "    sentence = f'{row[0]}'\n",
        "    tts.forward_with_urhythmic(sentence, wavs, save_file=True, save_example=True)\n",
        "\n",
        "\n",
        "print(\"All audio files have been generated and saved to the folder in your drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZ7GBNjXTAhb"
      },
      "source": [
        "# Testing and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxV83ekgTF00"
      },
      "source": [
        "To evaluate the zero-shot model they used the LibriSpeech test-clean dataset for target speaker reference utterances (ground truth). The database has speech from 20 males and 20 females, 8 minutes of speech per each. We downloaded the data and we specifically need the following file: test-clean, which contains subfolders (one for each speaker) then subfolders within those (one for each chapter of a book that the speakers read from), then the individual audio files (in .flac form, each file is a sentence from the chapter).\n",
        "\n",
        "\\\\\n",
        "\n",
        "To create the output for the model, they passed in 100 English sentences for each speaker, from the FLoRes+ dataset. We downloaded the data and figured out where to find the sentences. We really only need one file, “devtest.eng_Latn”, which contains a multitude of random English sentences. Below you will find example sentences.\n",
        "\n",
        "\\\\\n",
        "\n",
        "MOS = mean opinion score is a measure of the human-judged overall quality of an event or experience. For us, a MOS is a ranking of the quality of speech utterances. Most often judged on a scale of 1 (bad) to 5 (excellent), MOS’s are the average of a number of other human-scored individual parameters. Although originally MOS’s were derived from surveys of expert observers, today a MOS is often produced by an Objective Measurement Method, approximating a human ranking. 4.3-4.5 is considered an excellent target to shoot for due to human tendency to rarely give out perfect 5’s. Below 3.5 is generally unacceptable.\n",
        "\n",
        "\\\\\n",
        "\n",
        "All tests are conducted with $λ=1$. The evaluation focuses on a few key metrics of language:\n",
        "\n",
        "- **Naturalness: UTMOS**\n",
        " - UTMOS = UTokyo-SaruLab Mean Opinion Score, an autonomous method of calculating MOS.\n",
        "\n",
        "- **Intelligibility: WER, PER**\n",
        " - WER = Word Error Rate, i.e. the ratio of word errors in a transcript to the total words spoken. A lower WER in speech-to-text means better accuracy in recognizing speech. In our case, this would be calculated with the formula $\\frac{S+D+I}{N}$, where S is the number of substitutions (instances where a word in the synthesized sentence vector would need to be subsituted to match the truth vector), D is the number of deletions (instances where a word in the synthesized sentence vector would need to be deleted to match the truth vector), I is the number of insertions (instances where a new word would need to be inserted to match the truth vector), and N is the total number of phenomes. The numerator is also known as the edit distance because it represents \"how far away\" two sentences are.\n",
        " - PER = Phenome Error Rate, i.e. the ratio of phenome errors in a transcript to the total phenomes spoken. As above, a lower PER means better accuracy. The formula the same as above, except in the context of comparing phenomes instead of words.\n",
        " - Both of these are calculated using the Whisper-Large v3 model.\n",
        "\n",
        "- **Speaker Similarity: SECS**\n",
        " - SECS = Speaker-Encoder Cosine Similarity, i.e. the cosine similarity between the embeddings of two audio samples, which in our case are a ground truth sample from one speaker and the synthesized sample for that same speaker. The original paper uses ECAPA2 to find these embeddings and their similtarity. The goal of speaker similarity is to determine if two audio samples come from the same spaker, so if the output of the model is above a certain threshold, they are considered to be from the same speaker, otherwise, they are from different speakers.\n",
        "\n",
        "- **Subjective Evaluation: N-MOS, S-MOS**\n",
        " - N-MOS = Natural MOS, i.e. how natural the utterance (output) sounds compared to the ground-truth recording.\n",
        " - S-MOS = Similarity MOS, i.e. how similar the utterance sounds compared to the ground-truth recording.\n",
        "The original paper had 10 raters go through 3 synthesized sentences per speaker, thus they went through 60 in total. They then gave a score for each synthesis from 1 to 5 in 0.5 increments. They hired native English speakers in the United States through Amazon Mechanical Turk to rate, so in our case it would just be us 4 rating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8HfRtJ1oeh5",
        "outputId": "eab25c00-7ec2-4baf-a5f4-e858e1f815fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38yKvEQacy4k"
      },
      "source": [
        "## UTMOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "D4JRDmPMdT5C",
        "outputId": "e045a583-fa58-41c0-d574-6d359ed91000"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pip==23.2.1\n",
            "  Downloading pip-23.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/2.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/2.1 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-23.2.1\n",
            "Collecting utmos\n",
            "  Obtaining dependency information for utmos from https://files.pythonhosted.org/packages/45/2b/92e89033000755d437239da84e062eeeae464cbaafa2bc52ff028c609b84/utmos-1.1.10-py3-none-any.whl.metadata\n",
            "  Downloading utmos-1.1.10-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from utmos) (1.22.0)\n",
            "Collecting fairseq (from utmos)\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cached-path (from utmos)\n",
            "  Obtaining dependency information for cached-path from https://files.pythonhosted.org/packages/ba/39/861cf3eeebba6cfc88fe6eae6b5eb4eacc5385f3e303837064680453f8cb/cached_path-1.6.5-py3-none-any.whl.metadata\n",
            "  Downloading cached_path-1.6.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from utmos) (8.1.7)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.10/dist-packages (from utmos) (2.5.1+cu121)\n",
            "Collecting pytorch-lightning (from utmos)\n",
            "  Obtaining dependency information for pytorch-lightning from https://files.pythonhosted.org/packages/2b/d2/ecd65ff1e0b1ca79f9785dd65d5ced7ec2643a828068aaa24e47e4c84a14/pytorch_lightning-2.4.0-py3-none-any.whl.metadata\n",
            "  Downloading pytorch_lightning-2.4.0-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from utmos) (4.46.3)\n",
            "Requirement already satisfied: requests<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cached-path->utmos) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0,>=12.1 in /usr/local/lib/python3.10/dist-packages (from cached-path->utmos) (13.9.4)\n",
            "Requirement already satisfied: filelock<4.0,>=3.4 in /usr/local/lib/python3.10/dist-packages (from cached-path->utmos) (3.16.1)\n",
            "Collecting boto3<2.0,>=1.0 (from cached-path->utmos)\n",
            "  Obtaining dependency information for boto3<2.0,>=1.0 from https://files.pythonhosted.org/packages/0a/72/f6724a19acaac7a7cdfc088ac95d2d0ea3626c00d5a5197a99e49bde474d/boto3-1.35.80-py3-none-any.whl.metadata\n",
            "  Downloading boto3-1.35.80-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path->utmos) (2.8.0)\n",
            "Requirement already satisfied: huggingface-hub<0.27.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from cached-path->utmos) (0.26.5)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->utmos) (1.17.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->utmos) (3.0.11)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq->utmos)\n",
            "  Obtaining dependency information for hydra-core<1.1,>=1.0.7 from https://files.pythonhosted.org/packages/5f/2a/9c698daa12ed6e09e7629e6908528f043fa9de8a441c56cc13608d765fb2/hydra_core-1.0.7-py3-none-any.whl.metadata\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting omegaconf<2.1 (from fairseq->utmos)\n",
            "  Obtaining dependency information for omegaconf<2.1 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl.metadata\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq->utmos) (2024.9.11)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq->utmos)\n",
            "  Obtaining dependency information for sacrebleu>=1.4.12 from https://files.pythonhosted.org/packages/15/d8/e51d35bc863caa19ddeae48dfb890581a19326973ad1c9fa5dcfc63310f7/sacrebleu-2.4.3-py3-none-any.whl.metadata\n",
            "  Downloading sacrebleu-2.4.3-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq->utmos) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq->utmos) (4.66.6)\n",
            "Collecting bitarray (from fairseq->utmos)\n",
            "  Obtaining dependency information for bitarray from https://files.pythonhosted.org/packages/17/33/c2a7cb6f0030ea94408c84c4f80f4065b54b2bf1d4080e36fcd0b4c587a2/bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->utmos) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->utmos) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->utmos) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->utmos) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->utmos) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->fairseq->utmos) (1.3.0)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->utmos) (6.0.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning->utmos)\n",
            "  Obtaining dependency information for torchmetrics>=0.7.0 from https://files.pythonhosted.org/packages/46/1a/9728a502f377ab8cff1fd15c625aa2919a183fa113ebcefa2cd38edff28b/torchmetrics-1.6.0-py3-none-any.whl.metadata\n",
            "  Downloading torchmetrics-1.6.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning->utmos) (24.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning->utmos)\n",
            "  Obtaining dependency information for lightning-utilities>=0.10.0 from https://files.pythonhosted.org/packages/85/f3/1305321a12c984405e26fc64b5d521569e9872fb811f4aace8e168099160/lightning_utilities-0.11.9-py3-none-any.whl.metadata\n",
            "  Downloading lightning_utilities-0.11.9-py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers->utmos) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->utmos) (0.4.5)\n",
            "Collecting botocore<1.36.0,>=1.35.80 (from boto3<2.0,>=1.0->cached-path->utmos)\n",
            "  Obtaining dependency information for botocore<1.36.0,>=1.35.80 from https://files.pythonhosted.org/packages/f6/cc/7ecef0f0e883f4bd8e23a04e86d98f8c7d6aa6a821efcec67b3547388d2e/botocore-1.35.80-py3-none-any.whl.metadata\n",
            "  Downloading botocore-1.35.80-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2.0,>=1.0->cached-path->utmos)\n",
            "  Obtaining dependency information for jmespath<2.0.0,>=0.7.1 from https://files.pythonhosted.org/packages/31/b4/b9b800c45527aadd64d5b442f9b932b00648617eb5d63d2c7a6587b7cafc/jmespath-1.0.1-py3-none-any.whl.metadata\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2.0,>=1.0->cached-path->utmos)\n",
            "  Obtaining dependency information for s3transfer<0.11.0,>=0.10.0 from https://files.pythonhosted.org/packages/66/05/7957af15543b8c9799209506df4660cba7afc4cf94bfb60513827e96bed6/s3transfer-0.10.4-py3-none-any.whl.metadata\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec->torch->fairseq->utmos) (3.11.10)\n",
            "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (2.27.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (2.19.2)\n",
            "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (2.4.1)\n",
            "Requirement already satisfied: google-resumable-media>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (2.7.2)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->utmos)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning->utmos) (75.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cached-path->utmos) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cached-path->utmos) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cached-path->utmos) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.0->cached-path->utmos) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.1->cached-path->utmos) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0,>=12.1->cached-path->utmos) (2.18.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq->utmos)\n",
            "  Obtaining dependency information for portalocker from https://files.pythonhosted.org/packages/3d/4c/4cb6bb4061910ac74c444be76e7d17dba97d9057030cca2f96947c3f7a0f/portalocker-3.0.0-py3-none-any.whl.metadata\n",
            "  Downloading portalocker-3.0.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->utmos) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq->utmos)\n",
            "  Obtaining dependency information for colorama from https://files.pythonhosted.org/packages/d1/d6/3965ed04c63042e047cb6a3e6ed1a63a35087b6a609aa3a15ed8ac56c221/colorama-0.4.6-py2.py3-none-any.whl.metadata\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->utmos) (5.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->utmos) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec->torch->fairseq->utmos) (1.18.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.80->boto3<2.0,>=1.0->cached-path->utmos) (2.8.2)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (1.66.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (4.25.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (1.25.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (4.9)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.10/dist-packages (from google-resumable-media>=2.3.2->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (1.6.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.1->cached-path->utmos) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq->utmos) (3.0.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage<3.0,>=1.32.0->cached-path->utmos) (0.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.80->boto3<2.0,>=1.0->cached-path->utmos) (1.17.0)\n",
            "Downloading utmos-1.1.10-py3-none-any.whl (9.7 kB)\n",
            "Downloading cached_path-1.6.5-py3-none-any.whl (35 kB)\n",
            "Downloading pytorch_lightning-2.4.0-py3-none-any.whl (815 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m815.2/815.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.80-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.11.9-py3-none-any.whl (28 kB)\n",
            "Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Downloading sacrebleu-2.4.3-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.0/104.0 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.6.0-py3-none-any.whl (926 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m926.4/926.4 kB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.80-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.0.0-py3-none-any.whl (19 kB)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11288626 sha256=f4956040f2cc5c8fbb3341399e1056dcad4cbe941e74728f2d6ecfa8a1dfc4ea\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141214 sha256=46659f06af5f67a01acc34070a3c340c50d541901766067ad577ab6e31469870\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "\u001b[33mDEPRECATION: omegaconf 2.0.6 has a non-standard dependency specifier PyYAML>=5.1.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of omegaconf or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, lightning-utilities, jmespath, colorama, sacrebleu, hydra-core, botocore, torchmetrics, s3transfer, fairseq, boto3, pytorch-lightning, cached-path, utmos\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-3.0.0 boto3-1.35.80 botocore-1.35.80 cached-path-1.6.5 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 jmespath-1.0.1 lightning-utilities-0.11.9 omegaconf-2.0.6 portalocker-3.0.0 pytorch-lightning-2.4.0 s3transfer-0.10.4 sacrebleu-2.4.3 torchmetrics-1.6.0 utmos-1.1.10\n"
          ]
        }
      ],
      "source": [
        "!pip install pip==23.2.1\n",
        "!pip install utmos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3b9AQCBadHnm",
        "outputId": "22bc2f39-f09e-4fad-b289-7d0e1d86f96c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.9 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/huggingface/hub/models--mosnets--utmos/snapshots/65956d677bd502519c30c0f4bfda97a749d63009/model.ckpt`\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/checkpoint_utils.py:315: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state = torch.load(f, map_location=torch.device(\"cpu\"))\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "p225_gtts_generated_waveform_00.wav: 3.729201376438141\n",
            "LJ_generated_waveform_urythmic_00.wav: 4.256206393241882\n",
            "LJ_generated_waveform_00.wav: 3.794767916202545\n",
            "p225_gtts_generated_waveform_urythmic_00.wav: 3.5768465399742126\n",
            "p231_gtts_generated_waveform_urythmic_00.wav: 3.8537667393684387\n",
            "p228_gtts_generated_waveform_00.wav: 4.123617649078369\n",
            "p228_gtts_generated_waveform_urythmic_00.wav: 4.067911148071289\n",
            "p268_gtts_generated_waveform_urythmic_00.wav: 4.081933617591858\n",
            "p257_gtts_generated_waveform_00.wav: 3.787180006504059\n",
            "p257_gtts_generated_waveform_urythmic_00.wav: 3.4753727316856384\n",
            "p231_gtts_generated_waveform_00.wav: 3.9803765416145325\n",
            "p268_gtts_generated_waveform_00.wav: 4.387761950492859\n",
            "LJSpeech_examples: 3.7928078508377077\n",
            "LJSpeech_examples_urhythmic: 4.193322395185629\n",
            "p225_examples: 3.678368322253227\n",
            "p225_examples_urhythmic: 3.3803739565610886\n",
            "p228_examples: 3.9510156413167716\n",
            "p228_examples_urhythmic: 3.821045938928922\n",
            "p231_examples: 3.995636603931586\n",
            "p231_examples_urhythmic: 3.7087179525693257\n",
            "p257_examples: 3.7958769257863363\n",
            "p257_examples_urhythmic: 3.6377193146944045\n",
            "p268_examples: 3.9404078805446625\n",
            "p268_examples_urhythmic: 3.830675287246704\n",
            "3.8590188707783817\n",
            "3.7619758075310124\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import utmos\n",
        "\n",
        "# Initialize the model once\n",
        "model = utmos.Score()\n",
        "\n",
        "# Suppose you have a parent directory that contains the 14 subfolders\n",
        "parent_dir = '/content/drive/MyDrive/examples'\n",
        "\n",
        "# Get the list of all folders inside the parent directory\n",
        "folders = [os.path.join(parent_dir, d) for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]\n",
        "\n",
        "# Create an empty list to store results\n",
        "results = []\n",
        "i = 0\n",
        "# Iterate through each folder\n",
        "for folder_path in folders:\n",
        "    # Get the folder name (last part of the path)\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "\n",
        "    # Iterate through each .wav file in this folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith('.wav'):\n",
        "            audio_file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Calculate the score for this audio file\n",
        "            score = model.calculate_wav_file(audio_file_path)\n",
        "            if i % 75 == 0:\n",
        "              print(f'{filename}: {score}')\n",
        "            i += 1\n",
        "            # Append the result as a dictionary\n",
        "            results.append({\n",
        "                'folder': folder_name,\n",
        "                'filename': filename,\n",
        "                'score': score\n",
        "            })\n",
        "\n",
        "# Convert the list of dictionaries into a DataFrame\n",
        "df2 = pd.DataFrame(results)\n",
        "\n",
        "# Now df contains all the results for each wav file across all 14 folders.\n",
        "# If you want to save this DataFrame to a CSV file:\n",
        "folder_means = df.groupby('folder')['score'].mean()\n",
        "i = 0\n",
        "base = 0\n",
        "urhyth = 0\n",
        "# Print each folder and its mean score\n",
        "for folder, mean_score in folder_means.items():\n",
        "    print(f\"{folder}: {mean_score}\")\n",
        "\n",
        "    if i % 2 == 0:\n",
        "      base += mean_score\n",
        "    else:\n",
        "      urhyth += mean_score\n",
        "    i += 1\n",
        "print(base/6)\n",
        "print(urhyth/6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzmYeHsmc5Qc"
      },
      "source": [
        "## SECS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qK7HcKy42yAg",
        "outputId": "602cd7b4-7c81-453b-ec22-04e36f333575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.26.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade huggingface_hub\n",
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSGvt4Aj2oqc",
        "outputId": "0697f294-fde5-48ee-d168-23995cba7d1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LJSpeech_examples_gtts/LJ_generated_waveform_00.wav: [[0.56690031]]\n",
            "LJSpeech_examples_urhythmic_gtts/LJ_generated_waveform_urythmic_00.wav: [[0.66848637]]\n",
            "p225_examples_gtts/p225_gtts_generated_waveform_00.wav: [[0.48326665]]\n",
            "p225_examples_urhythmic_gtts/p225_gtts_generated_waveform_urythmic_00.wav: [[0.53131772]]\n",
            "p228_examples_urhythmic_gtts/p228_gtts_generated_waveform_urythmic_00.wav: [[0.56336158]]\n",
            "p228_examples_gtts/p228_gtts_generated_waveform_00.wav: [[0.51761073]]\n",
            "p231_examples_urhythmic_gtts/p231_gtts_generated_waveform_urythmic_00.wav: [[0.55573299]]\n",
            "p231_examples_gtts/p231_gtts_generated_waveform_00.wav: [[0.51919524]]\n",
            "p257_examples_urhythmic_gtts/p257_gtts_generated_waveform_urythmic_00.wav: [[0.53735418]]\n",
            "p257_examples_gtts/p257_gtts_generated_waveform_00.wav: [[0.54565198]]\n",
            "p268_examples_urhythmic_gtts/p268_gtts_generated_waveform_urythmic_00.wav: [[0.51130244]]\n",
            "p268_examples_gtts/p268_gtts_generated_waveform_00.wav: [[0.46486592]]\n"
          ]
        }
      ],
      "source": [
        "# Note: This metric needs a ground truth audio file to compare with,\n",
        "# so if you wish to test it, you must load up the LJSpeech dataset\n",
        "from huggingface_hub import hf_hub_download\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize the model once\n",
        "model_file = hf_hub_download(repo_id='Jenthe/ECAPA2', filename='ecapa2.pt', cache_dir=None)\n",
        "ecapa2 = torch.jit.load(model_file, map_location='cuda')\n",
        "ecapa2.half()\n",
        "\n",
        "parent_dir = '/content/drive/MyDrive/examples'\n",
        "\n",
        "folders = [os.path.join(parent_dir, d) for d in os.listdir(parent_dir) if os.path.isdir(os.path.join(parent_dir, d))]\n",
        "\n",
        "results = []\n",
        "i = 0\n",
        "\n",
        "for folder_path in folders:\n",
        "    folder_name = os.path.basename(folder_path)\n",
        "\n",
        "    # Iterate through each .wav file in this folder\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.lower().endswith('.wav'):\n",
        "            audio_file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            # Calculate the embedding for this audio file\n",
        "            audio_gen, sr = torchaudio.load(audio_file_path)# sample rate of 16 kHz expected\n",
        "            audio_gen = audio_gen.to('cuda')\n",
        "            embedding_generated = ecapa2(audio_gen).detach().cpu().numpy()\n",
        "\n",
        "            #Calculate the embedding for the desired speaker\n",
        "            if folder_name[:2] == 'LJ':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/LJ001-0001.wav')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p225':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p225/p225_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p228':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p228/p228_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p231':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p231/p231_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p232':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p232/p232_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p257':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p257/p257_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            elif folder_name[:4] == 'p268':\n",
        "              audio_truth, sr = torchaudio.load('/content/drive/MyDrive/p268/p268_001_mic1.flac')\n",
        "              audio_truth = audio_truth.to('cuda')\n",
        "              embedding_truth = ecapa2(audio_truth).detach().cpu().numpy()\n",
        "\n",
        "            #Calculate SECS (speaker encoder cosine similarity)\n",
        "            secs = cosine_similarity(embedding_generated, embedding_truth)\n",
        "\n",
        "            #the values in the paper are b/w 0 and 1 so normalize: min value is -1 and max value is 1\n",
        "            secs = (secs+1) / 2\n",
        "\n",
        "            #Check results\n",
        "            if i % 75 == 0:\n",
        "              print(f'{folder_name}/{filename}: {secs}')\n",
        "            i += 1\n",
        "\n",
        "            # Append the result as a dictionary\n",
        "            results.append({\n",
        "                'folder': folder_name,\n",
        "                'filename': filename,\n",
        "                'secs': secs\n",
        "            })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6708bH_KT-lk",
        "outputId": "a03001fc-f0cb-4be6-a62d-f3dfa18759c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (2.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding\n",
        "\n",
        "!pip install --upgrade pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2SiGxHuMTm-",
        "outputId": "21e4ca4c-8985-48d1-d362-569fbb379bb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LJSpeech_examples_gtts: 0.5706962302208631\n",
            "LJSpeech_examples_urhythmic_gtts: 0.6494904270143566\n",
            "p225_examples_gtts: 0.4821260035306302\n",
            "p225_examples_urhythmic_gtts: 0.531314990573211\n",
            "p228_examples_gtts: 0.5132718528655164\n",
            "p228_examples_urhythmic_gtts: 0.5460057999979038\n",
            "p231_examples_gtts: 0.5024797379488829\n",
            "p231_examples_urhythmic_gtts: 0.5428143174305543\n",
            "p257_examples_gtts: 0.5547537291982848\n",
            "p257_examples_urhythmic_gtts: 0.5399783076574057\n",
            "p268_examples_gtts: 0.47704823177929495\n",
            "p268_examples_urhythmic_gtts: 0.51124499872964\n",
            "0.5167292975905787\n",
            "0.5534748069005119\n",
            "All scores have been calculated and saved to: /content/no_taco_secs_scores.csv\n"
          ]
        }
      ],
      "source": [
        "df3 = pd.DataFrame(results)\n",
        "df3['secs'] = df3['secs'].apply(lambda x: x[0][0])\n",
        "df3.columns = list(df3.columns)\n",
        "\n",
        "# Now df contains all the results for each wav file across all 14 folders.\n",
        "# If you want to save this DataFrame to a CSV file:\n",
        "folder_means = df3.groupby('folder')['secs'].mean()\n",
        "i = 0\n",
        "base = 0\n",
        "urhyth = 0\n",
        "# Print each folder and its mean score\n",
        "for folder, mean_score in folder_means.items():\n",
        "    print(f\"{folder}: {mean_score}\")\n",
        "\n",
        "    if i % 2 == 0:\n",
        "      base += mean_score\n",
        "    else:\n",
        "      urhyth += mean_score\n",
        "    i += 1\n",
        "print(base/6)\n",
        "print(urhyth/6)\n",
        "\n",
        "output_csv_path = '/content/secs_scores.csv'\n",
        "df3_dict = df3.to_dict(orient='records')\n",
        "# Use the csv module to write the dictionary to CSV:\n",
        "import csv\n",
        "with open(output_csv_path, 'w', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=df3.columns)\n",
        "    writer.writeheader()\n",
        "    writer.writerows(df3_dict)\n",
        "\n",
        "print(\"All scores have been calculated and saved to:\", output_csv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlH3Q33zdBJj"
      },
      "source": [
        "## WER and PER"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JsWkhqPJLSHt",
        "outputId": "07203b62-2d11-490f-ad6a-e9b5eb5280da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.0/800.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (1.22.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (4.66.6)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper) (10.5.0)\n",
            "Collecting tiktoken (from openai-whisper)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting triton>=2.0.0 (from openai-whisper)\n",
            "  Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper) (3.16.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2.8.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803320 sha256=0a72b9f42f7e815f918420eb05fdff9f1fa4f0647faf5ab9b0fc15ad9a13c698\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/4a/1f/d1c4bf3b9133c8168fe617ed979cab7b14fe381d059ffb9d83\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, tiktoken, openai-whisper\n",
            "Successfully installed openai-whisper-20240930 tiktoken-0.8.0 triton-3.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install -U openai-whisper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "s6HIwkN2SyWi",
        "outputId": "b8f152bf-f1bc-46e6-c139-2d9fde6d0047"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting g2p-en\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.7)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (1.22.0)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (3.9.1)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p-en) (7.4.0)\n",
            "Collecting distance>=0.1.3 (from g2p-en)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p-en) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p-en) (4.4.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.2.4->g2p-en) (4.66.6)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p-en) (4.12.2)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: distance\n",
            "  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=204e55cf0dab853935bf98fd5444f48252fe462e16d89038716dfac4d30faec4\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/bb/de/f71bf63559ea9a921059a5405806f7ff6ed612a9231c4a9309\n",
            "Successfully built distance\n",
            "Installing collected packages: distance, rapidfuzz, jiwer, g2p-en\n",
            "Successfully installed distance-0.1.3 g2p-en-2.1.0 jiwer-3.0.5 rapidfuzz-3.10.1\n"
          ]
        }
      ],
      "source": [
        "!pip install jiwer g2p-en\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j5gMz2SnU8E7",
        "outputId": "ab1eedf2-adae-4ba3-9046-482f7b8cb1c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: g2p_en in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (1.22.0)\n",
            "Requirement already satisfied: inflect>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (7.4.0)\n",
            "Requirement already satisfied: distance>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from g2p_en) (0.1.3)\n",
            "Requirement already satisfied: more-itertools>=8.5.0 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p_en) (10.5.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from inflect>=0.3.1->g2p_en) (4.4.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from typeguard>=4.0.1->inflect>=0.3.1->g2p_en) (4.12.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade nltk g2p_en\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RDLdtdftUjau",
        "outputId": "febb5584-c329-432f-a0d1-c9e9f2cb8600"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CudO5fAkMCFY",
        "outputId": "084616b5-2563-430f-852d-7a1f01bece22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LJSpeech_examples_gtts/LJ_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous, so give them nest space.\n",
            "LJSpeech_examples_urhythmic_gtts/LJ_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous, so give them their space.\n",
            "p225_examples_gtts/p225_gtts_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous, so give them their space.\n",
            "p225_examples_urhythmic_gtts/p225_gtts_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild and potentially dangerous, so give them their space.\n",
            "p228_examples_gtts/p228_gtts_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous. So give them their space.\n",
            "p228_examples_urhythmic_gtts/p228_gtts_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild and potentially dangerous. So give them their space.\n",
            "p231_examples_gtts/p231_gtts_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous, so give them their space.\n",
            "p231_examples_urhythmic_gtts/p231_gtts_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous, so give them their space.\n",
            "p257_examples_gtts/p257_gtts_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous. So give them their space.\n",
            "p257_examples_urhythmic_gtts/p257_gtts_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild and potentially dangerous. So give them their space.\n",
            "p268_examples_gtts/p268_gtts_generated_waveform_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous. So give them their space.\n",
            "p268_examples_urhythmic_gtts/p268_gtts_generated_waveform_urythmic_00.wav: Each year, dozens of visitors are injured because they didn't keep a proper distance. These animals are large, wild, and potentially dangerous. So give them their space.\n",
            "                  speaker                      filename  \\\n",
            "0  LJSpeech_examples_gtts  LJ_generated_waveform_00.wav   \n",
            "1  LJSpeech_examples_gtts  LJ_generated_waveform_01.wav   \n",
            "2  LJSpeech_examples_gtts  LJ_generated_waveform_02.wav   \n",
            "3  LJSpeech_examples_gtts  LJ_generated_waveform_03.wav   \n",
            "4  LJSpeech_examples_gtts  LJ_generated_waveform_04.wav   \n",
            "\n",
            "                                       transcription  \\\n",
            "0  Each year, dozens of visitors are injured beca...   \n",
            "1  personal involvement, and continuing relations...   \n",
            "2  Women did the cooking in the yard. Stores were...   \n",
            "3  The main Amazon River is 6,387 kilometres, 3,9...   \n",
            "4  For some, understanding something about how ai...   \n",
            "\n",
            "                                        ground_truth  \n",
            "0  \"Each year, dozens of visitors are injured bec...  \n",
            "1  \"\"\"Personal involvement” and “continuing relat...  \n",
            "2  Women did the cooking in the yard; stores were...  \n",
            "3  \"The main Amazon River is 6,387 kilometers (3,...  \n",
            "4  \"For some, understanding something about how a...  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import whisper\n",
        "\n",
        "# Load the Whisper model once\n",
        "model = whisper.load_model(\"turbo\")\n",
        "\n",
        "# Path to the top-level directory containing all speaker folders\n",
        "data_dir = \"/content/drive/MyDrive/examples\"  # Adjust this to your directory\n",
        "\n",
        "# Load ground truth sentences from a text file\n",
        "# Each line in the file corresponds to a sentence, in the exact order as the clips\n",
        "with open(\"/content/sampled_sentences2.csv\", \"r\", encoding=\"utf-8\") as f:\n",
        "    ground_truth_sentences = [line.strip() for line in f if line.strip()]\n",
        "\n",
        "# Initialize a list for DataFrame rows\n",
        "rows = []\n",
        "\n",
        "# We'll assume each speaker directory has 75 files named consistently.\n",
        "# If the files have a known pattern like clip1.wav, clip2.wav, etc., we can rely on indexing.\n",
        "# If filenames differ, you may need to sort them alphabetically or by a numeric pattern.\n",
        "for speaker_dir in sorted(os.listdir(data_dir)):\n",
        "    speaker_path = os.path.join(data_dir, speaker_dir)\n",
        "    if os.path.isdir(speaker_path):\n",
        "        # Collect all wav files\n",
        "        audio_files = [f for f in os.listdir(speaker_path) if f.lower().endswith(\".wav\")]\n",
        "\n",
        "        # Sort them so that the order matches the ground truth sentences order\n",
        "        # This step assumes the naming pattern aligns so sorting gives the correct order.\n",
        "        # For example, if they are named clip1.wav, clip2.wav... sorting by name should be fine.\n",
        "        audio_files.sort()\n",
        "\n",
        "        # Ensure we have the same number of audio files as ground truth lines (in this case, 75)\n",
        "        for i, fname in enumerate(audio_files):\n",
        "            filepath = os.path.join(speaker_path, fname)\n",
        "\n",
        "            # Transcribe the audio file\n",
        "            result = model.transcribe(filepath)\n",
        "            transcription = result[\"text\"].strip()\n",
        "            if i % 75 == 0:\n",
        "              print(f'{speaker_dir}/{fname}: {transcription}')\n",
        "\n",
        "            # Match ground truth by index\n",
        "            # i corresponds to the i-th file of the speaker, so ground_truth_sentences[i] should match\n",
        "            # if the order is consistent.\n",
        "            gt_sentence = ground_truth_sentences[i] if i < len(ground_truth_sentences) else \"\"\n",
        "\n",
        "            rows.append({\n",
        "                \"speaker\": speaker_dir,\n",
        "                \"filename\": fname,\n",
        "                \"transcription\": transcription,\n",
        "                \"ground_truth\": gt_sentence\n",
        "            })\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(rows, columns=[\"speaker\", \"filename\", \"transcription\", \"ground_truth\"])\n",
        "\n",
        "# Preview the first few rows\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2_cnTdgSfLf"
      },
      "outputs": [],
      "source": [
        "df[\"ground_truth\"] = df[\"ground_truth\"].str.replace(\"[()]\", \"\", regex=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "VWcgLHViRgL6",
        "outputId": "df416623-ac00-4b73-f213-5f12f30eef9a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 900,\n  \"fields\": [\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"p268_examples_gtts\",\n          \"p257_examples_urhythmic_gtts\",\n          \"LJSpeech_examples_gtts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 900,\n        \"samples\": [\n          \"LJ_generated_waveform_70.wav\",\n          \"p268_gtts_generated_waveform_urythmic_02.wav\",\n          \"p225_gtts_generated_waveform_urythmic_06.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcription\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 240,\n        \"samples\": [\n          \"Most districts are served by small Japanese coaster buses, which are comfortable and sturdy.\",\n          \"When you went abroad at first, people were probably patient and understanding, knowing that travelers in a new country need to adapt.\",\n          \"around three minutes into the launch. An onboard camera showed numerous pieces of insulation foam break away from the fuel tank.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"\\\"For some, understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control.\\\"\",\n          \"Feral children may have experienced severe child abuse or trauma before being abandoned or running away.\",\n          \"\\\"The park covers 19,500 square kilometers and is divided in 14 different ecozones, each supporting different wildlife.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcription_clean\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 118,\n        \"samples\": [\n          \"naturalists and philosophers focused on classical texts and in particular on the bible in latin\",\n          \"apia is the capital of samoa the town is on the island of upolu and has a population of just under 40000\",\n          \"for some understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth_clean\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"for some understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control\",\n          \"feral children may have experienced severe child abuse or trauma before being abandoned or running away\",\n          \"the park covers 19500 square kilometers and is divided in 14 different ecozones each supporting different wildlife\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-369e82e1-a7e6-4cb7-88f4-7aae671430b0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>filename</th>\n",
              "      <th>transcription</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>transcription_clean</th>\n",
              "      <th>ground_truth_clean</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_00.wav</td>\n",
              "      <td>Each year, dozens of visitors are injured beca...</td>\n",
              "      <td>\"Each year, dozens of visitors are injured bec...</td>\n",
              "      <td>each year dozens of visitors are injured becau...</td>\n",
              "      <td>each year dozens of visitors are injured becau...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_01.wav</td>\n",
              "      <td>personal involvement, and continuing relations...</td>\n",
              "      <td>\"\"\"Personal involvement” and “continuing relat...</td>\n",
              "      <td>personal involvement and continuing relationsh...</td>\n",
              "      <td>personal involvement and continuing relationsh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_02.wav</td>\n",
              "      <td>Women did the cooking in the yard. Stores were...</td>\n",
              "      <td>Women did the cooking in the yard; stores were...</td>\n",
              "      <td>women did the cooking in the yard stores were ...</td>\n",
              "      <td>women did the cooking in the yard stores were ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_03.wav</td>\n",
              "      <td>The main Amazon River is 6,387 kilometres, 3,9...</td>\n",
              "      <td>\"The main Amazon River is 6,387 kilometers 3,9...</td>\n",
              "      <td>the main amazon river is 6387 kilometers 3980 ...</td>\n",
              "      <td>the main amazon river is 6387 kilometers 3980 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_04.wav</td>\n",
              "      <td>For some, understanding something about how ai...</td>\n",
              "      <td>\"For some, understanding something about how a...</td>\n",
              "      <td>for some understanding something about how air...</td>\n",
              "      <td>for some understanding something about how air...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>p268_examples_urhythmic_gtts</td>\n",
              "      <td>p268_gtts_generated_waveform_urythmic_70.wav</td>\n",
              "      <td>Money can be exchanged at the only bank in the...</td>\n",
              "      <td>Money can be exchanged at the only bank in the...</td>\n",
              "      <td>money can be exchanged at the only bank in the...</td>\n",
              "      <td>money can be exchanged at the only bank in the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>896</th>\n",
              "      <td>p268_examples_urhythmic_gtts</td>\n",
              "      <td>p268_gtts_generated_waveform_urythmic_71.wav</td>\n",
              "      <td>The feathers structure suggests that they were...</td>\n",
              "      <td>\"\"\"The feathers' structure suggests that they ...</td>\n",
              "      <td>the feathers structure suggests that they were...</td>\n",
              "      <td>the feathers structure suggests that they were...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>897</th>\n",
              "      <td>p268_examples_urhythmic_gtts</td>\n",
              "      <td>p268_gtts_generated_waveform_urythmic_72.wav</td>\n",
              "      <td>handicraft products might be defined as antiqu...</td>\n",
              "      <td>\"\"\"Handicraft products might be defined as ant...</td>\n",
              "      <td>handicraft products might be defined as antiqu...</td>\n",
              "      <td>handicraft products might be defined as antiqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>898</th>\n",
              "      <td>p268_examples_urhythmic_gtts</td>\n",
              "      <td>p268_gtts_generated_waveform_urythmic_73.wav</td>\n",
              "      <td>However, a nationwide road network is not econ...</td>\n",
              "      <td>\"\"\"However, a nationwide road network is not e...</td>\n",
              "      <td>however a nationwide road network is not econo...</td>\n",
              "      <td>however a nationwide road network is not econo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>899</th>\n",
              "      <td>p268_examples_urhythmic_gtts</td>\n",
              "      <td>p268_gtts_generated_waveform_urythmic_74.wav</td>\n",
              "      <td>In developed countries, you seldom hear simila...</td>\n",
              "      <td>In developed countries you seldom hear similar...</td>\n",
              "      <td>in developed countries you seldom hear similar...</td>\n",
              "      <td>in developed countries you seldom hear similar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>900 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-369e82e1-a7e6-4cb7-88f4-7aae671430b0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-369e82e1-a7e6-4cb7-88f4-7aae671430b0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-369e82e1-a7e6-4cb7-88f4-7aae671430b0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-58363a6f-dcdc-4af9-93cd-4c2d8f85035c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-58363a6f-dcdc-4af9-93cd-4c2d8f85035c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-58363a6f-dcdc-4af9-93cd-4c2d8f85035c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dd8d0617-679d-4826-9275-08b95c878f7f\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dd8d0617-679d-4826-9275-08b95c878f7f button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                          speaker  \\\n",
              "0          LJSpeech_examples_gtts   \n",
              "1          LJSpeech_examples_gtts   \n",
              "2          LJSpeech_examples_gtts   \n",
              "3          LJSpeech_examples_gtts   \n",
              "4          LJSpeech_examples_gtts   \n",
              "..                            ...   \n",
              "895  p268_examples_urhythmic_gtts   \n",
              "896  p268_examples_urhythmic_gtts   \n",
              "897  p268_examples_urhythmic_gtts   \n",
              "898  p268_examples_urhythmic_gtts   \n",
              "899  p268_examples_urhythmic_gtts   \n",
              "\n",
              "                                         filename  \\\n",
              "0                    LJ_generated_waveform_00.wav   \n",
              "1                    LJ_generated_waveform_01.wav   \n",
              "2                    LJ_generated_waveform_02.wav   \n",
              "3                    LJ_generated_waveform_03.wav   \n",
              "4                    LJ_generated_waveform_04.wav   \n",
              "..                                            ...   \n",
              "895  p268_gtts_generated_waveform_urythmic_70.wav   \n",
              "896  p268_gtts_generated_waveform_urythmic_71.wav   \n",
              "897  p268_gtts_generated_waveform_urythmic_72.wav   \n",
              "898  p268_gtts_generated_waveform_urythmic_73.wav   \n",
              "899  p268_gtts_generated_waveform_urythmic_74.wav   \n",
              "\n",
              "                                         transcription  \\\n",
              "0    Each year, dozens of visitors are injured beca...   \n",
              "1    personal involvement, and continuing relations...   \n",
              "2    Women did the cooking in the yard. Stores were...   \n",
              "3    The main Amazon River is 6,387 kilometres, 3,9...   \n",
              "4    For some, understanding something about how ai...   \n",
              "..                                                 ...   \n",
              "895  Money can be exchanged at the only bank in the...   \n",
              "896  The feathers structure suggests that they were...   \n",
              "897  handicraft products might be defined as antiqu...   \n",
              "898  However, a nationwide road network is not econ...   \n",
              "899  In developed countries, you seldom hear simila...   \n",
              "\n",
              "                                          ground_truth  \\\n",
              "0    \"Each year, dozens of visitors are injured bec...   \n",
              "1    \"\"\"Personal involvement” and “continuing relat...   \n",
              "2    Women did the cooking in the yard; stores were...   \n",
              "3    \"The main Amazon River is 6,387 kilometers 3,9...   \n",
              "4    \"For some, understanding something about how a...   \n",
              "..                                                 ...   \n",
              "895  Money can be exchanged at the only bank in the...   \n",
              "896  \"\"\"The feathers' structure suggests that they ...   \n",
              "897  \"\"\"Handicraft products might be defined as ant...   \n",
              "898  \"\"\"However, a nationwide road network is not e...   \n",
              "899  In developed countries you seldom hear similar...   \n",
              "\n",
              "                                   transcription_clean  \\\n",
              "0    each year dozens of visitors are injured becau...   \n",
              "1    personal involvement and continuing relationsh...   \n",
              "2    women did the cooking in the yard stores were ...   \n",
              "3    the main amazon river is 6387 kilometers 3980 ...   \n",
              "4    for some understanding something about how air...   \n",
              "..                                                 ...   \n",
              "895  money can be exchanged at the only bank in the...   \n",
              "896  the feathers structure suggests that they were...   \n",
              "897  handicraft products might be defined as antiqu...   \n",
              "898  however a nationwide road network is not econo...   \n",
              "899  in developed countries you seldom hear similar...   \n",
              "\n",
              "                                    ground_truth_clean  \n",
              "0    each year dozens of visitors are injured becau...  \n",
              "1    personal involvement and continuing relationsh...  \n",
              "2    women did the cooking in the yard stores were ...  \n",
              "3    the main amazon river is 6387 kilometers 3980 ...  \n",
              "4    for some understanding something about how air...  \n",
              "..                                                 ...  \n",
              "895  money can be exchanged at the only bank in the...  \n",
              "896  the feathers structure suggests that they were...  \n",
              "897  handicraft products might be defined as antiqu...  \n",
              "898  however a nationwide road network is not econo...  \n",
              "899  in developed countries you seldom hear similar...  \n",
              "\n",
              "[900 rows x 6 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "\n",
        "normalizer = EnglishTextNormalizer()\n",
        "df[\"transcription_clean\"] = [normalizer(text) for text in df[\"transcription\"]]\n",
        "df[\"ground_truth_clean\"] = [normalizer(text) for text in df[\"ground_truth\"]]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "CryFzk01Szrs",
        "outputId": "71b09e87-4e68-4352-cceb-2cd601d933cb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 900,\n  \"fields\": [\n    {\n      \"column\": \"speaker\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"p268_examples_gtts\",\n          \"p257_examples_urhythmic_gtts\",\n          \"LJSpeech_examples_gtts\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filename\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 900,\n        \"samples\": [\n          \"LJ_generated_waveform_70.wav\",\n          \"p268_gtts_generated_waveform_urythmic_02.wav\",\n          \"p225_gtts_generated_waveform_urythmic_06.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcription\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 240,\n        \"samples\": [\n          \"Most districts are served by small Japanese coaster buses, which are comfortable and sturdy.\",\n          \"When you went abroad at first, people were probably patient and understanding, knowing that travelers in a new country need to adapt.\",\n          \"around three minutes into the launch. An onboard camera showed numerous pieces of insulation foam break away from the fuel tank.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"\\\"For some, understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control.\\\"\",\n          \"Feral children may have experienced severe child abuse or trauma before being abandoned or running away.\",\n          \"\\\"The park covers 19,500 square kilometers and is divided in 14 different ecozones, each supporting different wildlife.\\\"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"transcription_clean\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 118,\n        \"samples\": [\n          \"naturalists and philosophers focused on classical texts and in particular on the bible in latin\",\n          \"apia is the capital of samoa the town is on the island of upolu and has a population of just under 40000\",\n          \"for some understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ground_truth_clean\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          \"for some understanding something about how aircraft work and what happens during a flight may help to overcome a fear which is based on the unknown or on not being in control\",\n          \"feral children may have experienced severe child abuse or trauma before being abandoned or running away\",\n          \"the park covers 19500 square kilometers and is divided in 14 different ecozones each supporting different wildlife\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"wer\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04297419349056816,\n        \"min\": 0.0,\n        \"max\": 0.23076923076923078,\n        \"num_unique_values\": 28,\n        \"samples\": [\n          0.09090909090909091,\n          0.06976744186046512,\n          0.07692307692307693\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7368aaa9-df7c-4172-97a3-95b9585121cb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>speaker</th>\n",
              "      <th>filename</th>\n",
              "      <th>transcription</th>\n",
              "      <th>ground_truth</th>\n",
              "      <th>transcription_clean</th>\n",
              "      <th>ground_truth_clean</th>\n",
              "      <th>wer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_00.wav</td>\n",
              "      <td>Each year, dozens of visitors are injured beca...</td>\n",
              "      <td>\"Each year, dozens of visitors are injured bec...</td>\n",
              "      <td>each year dozens of visitors are injured becau...</td>\n",
              "      <td>each year dozens of visitors are injured becau...</td>\n",
              "      <td>0.035714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_01.wav</td>\n",
              "      <td>personal involvement, and continuing relations...</td>\n",
              "      <td>\"\"\"Personal involvement” and “continuing relat...</td>\n",
              "      <td>personal involvement and continuing relationsh...</td>\n",
              "      <td>personal involvement and continuing relationsh...</td>\n",
              "      <td>0.041667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_02.wav</td>\n",
              "      <td>Women did the cooking in the yard. Stores were...</td>\n",
              "      <td>Women did the cooking in the yard; stores were...</td>\n",
              "      <td>women did the cooking in the yard stores were ...</td>\n",
              "      <td>women did the cooking in the yard stores were ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_03.wav</td>\n",
              "      <td>The main Amazon River is 6,387 kilometres, 3,9...</td>\n",
              "      <td>\"The main Amazon River is 6,387 kilometers 3,9...</td>\n",
              "      <td>the main amazon river is 6387 kilometers 3980 ...</td>\n",
              "      <td>the main amazon river is 6387 kilometers 3980 ...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LJSpeech_examples_gtts</td>\n",
              "      <td>LJ_generated_waveform_04.wav</td>\n",
              "      <td>For some, understanding something about how ai...</td>\n",
              "      <td>\"For some, understanding something about how a...</td>\n",
              "      <td>for some understanding something about how air...</td>\n",
              "      <td>for some understanding something about how air...</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7368aaa9-df7c-4172-97a3-95b9585121cb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7368aaa9-df7c-4172-97a3-95b9585121cb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7368aaa9-df7c-4172-97a3-95b9585121cb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-02d515c3-f795-429f-a564-cd777a76f11d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02d515c3-f795-429f-a564-cd777a76f11d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-02d515c3-f795-429f-a564-cd777a76f11d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                  speaker                      filename  \\\n",
              "0  LJSpeech_examples_gtts  LJ_generated_waveform_00.wav   \n",
              "1  LJSpeech_examples_gtts  LJ_generated_waveform_01.wav   \n",
              "2  LJSpeech_examples_gtts  LJ_generated_waveform_02.wav   \n",
              "3  LJSpeech_examples_gtts  LJ_generated_waveform_03.wav   \n",
              "4  LJSpeech_examples_gtts  LJ_generated_waveform_04.wav   \n",
              "\n",
              "                                       transcription  \\\n",
              "0  Each year, dozens of visitors are injured beca...   \n",
              "1  personal involvement, and continuing relations...   \n",
              "2  Women did the cooking in the yard. Stores were...   \n",
              "3  The main Amazon River is 6,387 kilometres, 3,9...   \n",
              "4  For some, understanding something about how ai...   \n",
              "\n",
              "                                        ground_truth  \\\n",
              "0  \"Each year, dozens of visitors are injured bec...   \n",
              "1  \"\"\"Personal involvement” and “continuing relat...   \n",
              "2  Women did the cooking in the yard; stores were...   \n",
              "3  \"The main Amazon River is 6,387 kilometers 3,9...   \n",
              "4  \"For some, understanding something about how a...   \n",
              "\n",
              "                                 transcription_clean  \\\n",
              "0  each year dozens of visitors are injured becau...   \n",
              "1  personal involvement and continuing relationsh...   \n",
              "2  women did the cooking in the yard stores were ...   \n",
              "3  the main amazon river is 6387 kilometers 3980 ...   \n",
              "4  for some understanding something about how air...   \n",
              "\n",
              "                                  ground_truth_clean       wer  \n",
              "0  each year dozens of visitors are injured becau...  0.035714  \n",
              "1  personal involvement and continuing relationsh...  0.041667  \n",
              "2  women did the cooking in the yard stores were ...  0.000000  \n",
              "3  the main amazon river is 6387 kilometers 3980 ...  0.000000  \n",
              "4  for some understanding something about how air...  0.000000  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from jiwer import wer\n",
        "\n",
        "# Calculate WER for each row in the DataFrame\n",
        "df[\"wer\"] = [wer(ref, hyp) for ref, hyp in zip(df[\"ground_truth_clean\"], df[\"transcription_clean\"])]\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zz23XfUvTx9v",
        "outputId": "1b027965-fd17-488b-fcb5-3e19da55543d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
          ]
        }
      ],
      "source": [
        "from g2p_en import G2p\n",
        "\n",
        "g2p = G2p()\n",
        "\n",
        "def text_to_phonemes(text):\n",
        "    # Convert text to phonemes using g2p\n",
        "    phonemes = g2p(text)\n",
        "    # g2p returns a list of phonemes and possibly some punctuation\n",
        "    # Filter out non-phoneme tokens if needed:\n",
        "    phonemes = [p for p in phonemes if p.isalpha()]\n",
        "    return phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFSz_qtTUESD",
        "outputId": "369ef776-f7c7-4c43-db05-3349ddf055f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                  speaker                      filename  \\\n",
            "0  LJSpeech_examples_gtts  LJ_generated_waveform_00.wav   \n",
            "1  LJSpeech_examples_gtts  LJ_generated_waveform_01.wav   \n",
            "2  LJSpeech_examples_gtts  LJ_generated_waveform_02.wav   \n",
            "3  LJSpeech_examples_gtts  LJ_generated_waveform_03.wav   \n",
            "4  LJSpeech_examples_gtts  LJ_generated_waveform_04.wav   \n",
            "\n",
            "                                       transcription  \\\n",
            "0  Each year, dozens of visitors are injured beca...   \n",
            "1  personal involvement, and continuing relations...   \n",
            "2  Women did the cooking in the yard. Stores were...   \n",
            "3  The main Amazon River is 6,387 kilometres, 3,9...   \n",
            "4  For some, understanding something about how ai...   \n",
            "\n",
            "                                        ground_truth  \\\n",
            "0  \"Each year, dozens of visitors are injured bec...   \n",
            "1  \"\"\"Personal involvement” and “continuing relat...   \n",
            "2  Women did the cooking in the yard; stores were...   \n",
            "3  \"The main Amazon River is 6,387 kilometers 3,9...   \n",
            "4  \"For some, understanding something about how a...   \n",
            "\n",
            "                                 transcription_clean  \\\n",
            "0  each year dozens of visitors are injured becau...   \n",
            "1  personal involvement and continuing relationsh...   \n",
            "2  women did the cooking in the yard stores were ...   \n",
            "3  the main amazon river is 6387 kilometers 3980 ...   \n",
            "4  for some understanding something about how air...   \n",
            "\n",
            "                                  ground_truth_clean       wer       per  \n",
            "0  each year dozens of visitors are injured becau...  0.035714  0.044118  \n",
            "1  personal involvement and continuing relationsh...  0.041667  0.000000  \n",
            "2  women did the cooking in the yard stores were ...  0.000000  0.000000  \n",
            "3  the main amazon river is 6387 kilometers 3980 ...  0.000000  0.000000  \n",
            "4  for some understanding something about how air...  0.000000  0.000000  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "def compute_per_for_row(row):\n",
        "    # Convert the reference and hypothesis texts to phonemes\n",
        "    reference_phonemes = text_to_phonemes(row[\"ground_truth_clean\"])\n",
        "    hypothesis_phonemes = text_to_phonemes(row[\"transcription_clean\"])\n",
        "\n",
        "    # Join them into strings for jiwer\n",
        "    ref_phoneme_str = \" \".join(reference_phonemes)\n",
        "    hyp_phoneme_str = \" \".join(hypothesis_phonemes)\n",
        "\n",
        "    # Compute PER using wer function\n",
        "    return wer(ref_phoneme_str, hyp_phoneme_str)\n",
        "\n",
        "# Apply the function to each row of the DataFrame\n",
        "df[\"per\"] = df.apply(compute_per_for_row, axis=1)\n",
        "\n",
        "# Now 'df' will have a new column 'per' with the phoneme error rate\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32Lm7BflVm3h",
        "outputId": "5d7e2ef8-3cf6-4a0c-87da-c76ea1d8bbd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             speaker       wer       per\n",
            "0             LJSpeech_examples_gtts  0.015789  0.010536\n",
            "1   LJSpeech_examples_urhythmic_gtts  0.019772  0.010754\n",
            "2                 p225_examples_gtts  0.013540  0.008231\n",
            "3       p225_examples_urhythmic_gtts  0.016080  0.006540\n",
            "4                 p228_examples_gtts  0.019743  0.013647\n",
            "5       p228_examples_urhythmic_gtts  0.020906  0.011958\n",
            "6                 p231_examples_gtts  0.014669  0.008263\n",
            "7       p231_examples_urhythmic_gtts  0.021695  0.014283\n",
            "8                 p257_examples_gtts  0.019027  0.013242\n",
            "9       p257_examples_urhythmic_gtts  0.018525  0.010895\n",
            "10                p268_examples_gtts  0.016392  0.010644\n",
            "11      p268_examples_urhythmic_gtts  0.017118  0.010928\n"
          ]
        }
      ],
      "source": [
        "mean_results = df.groupby(\"speaker\")[[\"wer\", \"per\"]].mean().reset_index()\n",
        "print(mean_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUXBXg0qWIRB"
      },
      "outputs": [],
      "source": [
        "output_csv_path = '/content/wer_per_scores.csv'\n",
        "df.to_csv(output_csv_path, index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "rvMN1tiTqa9c",
        "HZVo4IxMeAGI",
        "I_-5vfgQX5SW"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0768730567584a70990f92e698dc2b19": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2928b6d68d4f45b9aa3c594e7daeb465": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "29e445fb5d7542b7a674745fc2751210": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea56e1b6309648aab718fac04d75c2df",
            "placeholder": "​",
            "style": "IPY_MODEL_dcbb88cc507048c0a6cd11a955b77725",
            "value": "config.json: 100%"
          }
        },
        "41b4387af59a46ed8c9895ba65de23be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8b6d9e60ccf4a7c98e0268ce5b80743",
            "placeholder": "​",
            "style": "IPY_MODEL_9d2ef5bc7fd841dbb0abb4869af4a325",
            "value": " 2.22k/2.22k [00:00&lt;00:00, 123kB/s]"
          }
        },
        "4aa2ab1316c74ec0a8321f3ccf893b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d33df9df60064ddd8c75a876ee2cfc6c",
            "placeholder": "​",
            "style": "IPY_MODEL_af952d4e13cf4f1da608bf5538d5e000",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "4ade8b6e915043768fc70bff89846938": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4aa2ab1316c74ec0a8321f3ccf893b2c",
              "IPY_MODEL_f44bd06757de43be8117478edde0bf43",
              "IPY_MODEL_d2d4b02220744d90b8ffc8c3599ec0fa"
            ],
            "layout": "IPY_MODEL_50e8ff7fc43b456ba4ec5709811bef20"
          }
        },
        "50e8ff7fc43b456ba4ec5709811bef20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ed898fb83064df69837236c2e22a32d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d463ca16b64476eb40aa29c649dc94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5ed898fb83064df69837236c2e22a32d",
            "max": 2222,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9169bd3a89d54422a722b9ce513743ba",
            "value": 2222
          }
        },
        "81311bceba6d47c69e92ab4acca3ffad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_29e445fb5d7542b7a674745fc2751210",
              "IPY_MODEL_6d463ca16b64476eb40aa29c649dc94a",
              "IPY_MODEL_41b4387af59a46ed8c9895ba65de23be"
            ],
            "layout": "IPY_MODEL_d4ac0eb374d0481ba7043e6163ba1502"
          }
        },
        "9169bd3a89d54422a722b9ce513743ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d2ef5bc7fd841dbb0abb4869af4a325": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f407e024f754cb1a5f8df0d811f8f00": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af952d4e13cf4f1da608bf5538d5e000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd4f6eb26d0e4e7d9cb88bd6e87b5f3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d2d4b02220744d90b8ffc8c3599ec0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f407e024f754cb1a5f8df0d811f8f00",
            "placeholder": "​",
            "style": "IPY_MODEL_2928b6d68d4f45b9aa3c594e7daeb465",
            "value": " 1.26G/1.26G [00:13&lt;00:00, 37.1MB/s]"
          }
        },
        "d33df9df60064ddd8c75a876ee2cfc6c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ac0eb374d0481ba7043e6163ba1502": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8b6d9e60ccf4a7c98e0268ce5b80743": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dcbb88cc507048c0a6cd11a955b77725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea56e1b6309648aab718fac04d75c2df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44bd06757de43be8117478edde0bf43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0768730567584a70990f92e698dc2b19",
            "max": 1261990257,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd4f6eb26d0e4e7d9cb88bd6e87b5f3b",
            "value": 1261990257
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
